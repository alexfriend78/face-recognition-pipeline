input {
  beats {
    port => 5044
  }
  
  # Direct JSON logs from the application
  tcp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # Parse Docker container logs
  if [container] {
    # Extract container name and service
    mutate {
      add_field => { "service_name" => "%{[container][name]}" }
    }
    
    # Remove docker container prefix if present
    if [service_name] =~ /^\/.*/ {
      mutate {
        gsub => [ "service_name", "^/", "" ]
      }
    }
  }
  
  # Parse JSON logs from our application
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "app_log"
    }
    
    # Extract common fields from structured logs
    if [app_log] {
      mutate {
        add_field => {
          "log_level" => "%{[app_log][level]}"
          "logger_name" => "%{[app_log][logger]}"
          "timestamp_app" => "%{[app_log][timestamp]}"
        }
      }
      
      # Extract request context if present
      if [app_log][request_id] {
        mutate {
          add_field => {
            "request_id" => "%{[app_log][request_id]}"
            "http_method" => "%{[app_log][method]}"
            "http_path" => "%{[app_log][path]}"
            "remote_addr" => "%{[app_log][remote_addr]}"
          }
        }
      }
      
      # Extract task context if present
      if [app_log][task_id] {
        mutate {
          add_field => {
            "task_id" => "%{[app_log][task_id]}"
            "task_name" => "%{[app_log][task_name]}"
            "task_retries" => "%{[app_log][task_retries]}"
          }
        }
      }
      
      # Extract performance metrics if present
      if [app_log][duration_seconds] {
        mutate {
          add_field => { "duration_seconds" => "%{[app_log][duration_seconds]}" }
          convert => { "duration_seconds" => "float" }
        }
      }
      
      # Extract operation context
      if [app_log][operation] {
        mutate {
          add_field => { "operation" => "%{[app_log][operation]}" }
        }
      }
    }
  }
  
  # Parse Python traceback/error logs
  if [message] =~ /Traceback \(most recent call last\)/ {
    mutate {
      add_field => { "log_type" => "python_traceback" }
      add_field => { "log_level" => "ERROR" }
    }
  }
  
  # Parse Celery logs
  if [message] =~ /\[.*\] Task/ {
    grok {
      match => { 
        "message" => "\[%{TIMESTAMP_ISO8601:celery_timestamp}\] %{WORD:celery_log_level}: %{GREEDYDATA:celery_message}" 
      }
    }
    
    if [celery_message] =~ /Task.*received/ {
      mutate { add_field => { "celery_event" => "task_received" } }
    } else if [celery_message] =~ /Task.*succeeded/ {
      mutate { add_field => { "celery_event" => "task_succeeded" } }
    } else if [celery_message] =~ /Task.*failed/ {
      mutate { add_field => { "celery_event" => "task_failed" } }
    }
  }
  
  # Parse Nginx access logs
  if [service_name] == "nginx" and [message] !~ /^\{.*\}$/ {
    grok {
      match => { 
        "message" => "%{IPORHOST:remote_ip} - %{DATA:remote_user} \[%{HTTPDATE:access_timestamp}\] \"%{WORD:http_method} %{DATA:http_path} HTTP/%{NUMBER:http_version}\" %{NUMBER:http_status} %{NUMBER:body_bytes} \"%{DATA:http_referer}\" \"%{DATA:user_agent}\"" 
      }
    }
    
    if [http_status] {
      mutate {
        convert => { 
          "http_status" => "integer"
          "body_bytes" => "integer"
        }
      }
    }
    
    # Categorize HTTP status codes
    if [http_status] >= 200 and [http_status] < 300 {
      mutate { add_field => { "http_status_class" => "success" } }
    } else if [http_status] >= 300 and [http_status] < 400 {
      mutate { add_field => { "http_status_class" => "redirect" } }
    } else if [http_status] >= 400 and [http_status] < 500 {
      mutate { add_field => { "http_status_class" => "client_error" } }
    } else if [http_status] >= 500 {
      mutate { add_field => { "http_status_class" => "server_error" } }
    }
  }
  
  # Add common fields
  mutate {
    add_field => { 
      "environment" => "development"
      "application" => "face-recognition-pipeline"
    }
  }
  
  # Parse timestamp if not already parsed
  if ![timestamp_app] and [@timestamp] {
    date {
      match => [ "@timestamp", "ISO8601" ]
    }
  }
  
  # Remove unnecessary fields to reduce storage
  mutate {
    remove_field => [ "agent", "input", "log", "ecs" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "face-recognition-logs-%{+YYYY.MM.dd}"
    
    # Create index template for better field mapping
    template_name => "face-recognition"
    template_pattern => "face-recognition-logs-*"
    template => {
      "mappings" => {
        "properties" => {
          "log_level" => { "type" => "keyword" }
          "service_name" => { "type" => "keyword" }
          "request_id" => { "type" => "keyword" }
          "task_id" => { "type" => "keyword" }
          "operation" => { "type" => "keyword" }
          "http_method" => { "type" => "keyword" }
          "http_status" => { "type" => "integer" }
          "duration_seconds" => { "type" => "float" }
          "@timestamp" => { "type" => "date" }
          "message" => { "type" => "text" }
        }
      }
    }
  }
  
  # Also output to stdout for debugging
  stdout {
    codec => rubydebug
  }
}

